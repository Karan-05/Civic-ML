```markdown
# Civic's IMDb Sentiment Classification with Knowledge Distillation

Two epochs of total training, one GPUâ€‘hour, **stateâ€‘ofâ€‘theâ€‘art accuracy** for freeâ€‘tier Colab users:

| Metric (IMDb test)          | **Teacher â€“ BERTâ€‘base** | **Student â€“ DistilBERT** | Î” Change    |
|-----------------------------|-------------------------|--------------------------|-------------|
| **Accuracy**                | **93.45â€¯%**             | 92.69â€¯%                  | â–¼â€¯0.76â€¯pp   |
| **Weighted F1**             | **0.9345**              | 0.9269                   | â–¼â€¯0.8â€¯pp    |
| **Latency**<sup>â€ </sup>     | 47.6â€¯ms                 | **24.2â€¯ms**              | **â€‘49â€¯%**   |
| **Throughput**<sup>â€ </sup>  | 2â€¯099â€¯samples/s         | **4â€¯139â€¯samples/s**      | **+97â€¯%**   |
| **Parameters**              | 109â€¯M                   | **67â€¯M**                 | **â€‘39â€¯%**   |
| **Disk size**               | 417â€¯MB                  | **255â€¯MB**               | **â€‘39â€¯%**   |

<sup>â€  Measured on an NVIDIA T4 with a 100â€‘sample batch.</sup>

The **student keeps ~99â€¯% of the teacherâ€™s F1** while being roughly **2Ã— faster** and **40â€¯% smaller**.

---

## ðŸ”§ Motivation & Choices

| Component               | Why this choice?                                                                                                                                           |
|-------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **IMDb dataset**        | 25â€¯k balanced movie reviews, binary sentiment â†’ quick to iterate, widely used benchmark.                                                                    |
| **BERTâ€‘base teacher**   | Strong outâ€‘ofâ€‘theâ€‘box language model; reaches >â€¯93â€¯% accuracy in a single epoch.                                                                           |
| **DistilBERT student**  | 6â€‘layer distilled BERT â†’ same vocab & architecture (easy KD), 40â€¯% fewer params, faster inference.                                                          |
| **Onâ€‘theâ€‘fly KD**       | Compute teacher logits inside `compute_loss` â†’ no extra dataset columns, minimal RAM overhead.                                                              |
| **Weightsâ€¯&â€¯Biases**    | Automatic experiment tracking: loss curves, metrics, confusion matrix, GPU/util stats â†’ reproducibility in one dashboard link without extra code. Works seamlessly via ðŸ¤—â€¯`Trainer` integration. |

---

## 1  Methodology

### 1.1 Data prep

```python
from datasets import load_dataset
ds = load_dataset("imdb")
ds["train"] = ds["train"].shuffle(seed=42)
split = ds["train"].train_test_split(test_size=0.1, seed=42)
train_ds, val_ds, test_ds = split["train"], split["test"], ds["test"]
````

### 1.2 Teacher fineâ€‘tune

* **Model**: `bertâ€‘baseâ€‘uncased`
* **Hyperâ€‘params**: 1â€¯epoch, batchâ€¯16, LRâ€¯2eâ€‘5, crossâ€‘entropy loss.

### 1.3 Student distillation

Loss function:

$$
\begin{aligned}
\mathcal{L}(\theta_s)
&= \alpha\,\mathrm{CE}(y, s) \\
&\quad + (1 - \alpha)\,T^2\,\mathrm{KL}\!\Bigl(
    \mathrm{softmax}\bigl(\tfrac{t}{T}\bigr)
    \,\big\|\,
    \mathrm{softmax}\bigl(\tfrac{s}{T}\bigr)
  \Bigr).
\end{aligned}
$$

* **Î± = 0.5**, **T = 2.0**
* Teacher logits computed **onâ€‘theâ€‘fly** inside `DistillTrainer.compute_loss`.

### 1.4 Evaluation & analysis

* **Accuracy / F1** via ðŸ¤—â€¯`evaluate`
* **Latency & throughput** with a simple timing loop on GPU/CPU
* **Parameter & disk size** via `model.numel()` + folder size
* Optional: probability **calibration curves** (`sklearn.calibration_curve`)

---

## 3  Results Discussion

* **Speed vs Accuracy tradeâ€‘off:** DistilBERT doubles throughput while losing <â€¯1â€¯pp accuracy â€“ ideal for highâ€‘QPS services.
* **Memory & disk:** Save \~â€¯160â€¯MB on disk and \~â€¯1.2â€¯GB peak GPU RAM during inference.
* **Cost & sustainability:** Fewer FLOPs â‡’ lower cloud bill and carbon footprint.
* **W\&B dashboard:** Reviewers verify curves & system metrics without rerunning code.

---

## 4  Potential Extensions

| Idea                                 | Benefit                               |
| ------------------------------------ | ------------------------------------- |
| Multiâ€‘epoch KD + LR warmâ€‘up          | Push student >â€¯93â€¯% accuracy.         |
| 8â€‘bit / 4â€‘bit quantization           | Further 50â€¯% size & 30â€¯% latency cut. |
| Data augmentation (backâ€‘translation) | Better OOD generalization.            |
| Hyperâ€‘param sweeps (Î±,â€¯T) in W\&B    | Find best calibration vs. accuracy.   |

---
```
```
